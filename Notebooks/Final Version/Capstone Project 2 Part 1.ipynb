{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tom Halloin <br> Springboard Data Science Career Track <br>\n",
    "\n",
    "<h1 align=\"center\">Capstone Project 2: Analysis of Berkshire Hathaway Shareholder Letters Using Natural Language Processing (NLP) Techniques</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T02:05:15.807118Z",
     "start_time": "2019-12-12T02:05:15.791255Z"
    }
   },
   "source": [
    "<h3 align=\"center\">Part 1: Scraping Data</h3> <br>\n",
    "\n",
    "The data comes from Berkshire Hathawayâ€™s shareholder letters available at the link https://www.berkshirehathaway.com/letters/letters.html. The letters come in both HTML and PDF format, so part of the challenge will be scraping data from both formats into a feasible data set.\n",
    "\n",
    "<b>NOTE: Berkshire eventually caught on that I was a bot and has denied me scraping access when looping across multiple files. I do have the raw letters in a file on my laptop from before the denial. In other words, look, but touch at your own peril.</b>\n",
    "\n",
    "<h4> Updated to include 2019 annual letter</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T18:53:32.925369Z",
     "start_time": "2020-02-22T18:53:32.351410Z"
    }
   },
   "outputs": [],
   "source": [
    "import certifi  # validates trustworthiness of site\n",
    "import pickle  # Opening and closing intermediate files\n",
    "import PyPDF2  # library to get data from PDFs\n",
    "import os  # operating system\n",
    "import re  # Regular expressions\n",
    "import requests  # libary for web scraping\n",
    "import shutil  # file operations\n",
    "import urllib3  # Get stuff from the internet\n",
    "import urllib3.contrib.pyopenssl  # provides secure connection to site\n",
    "from bs4 import BeautifulSoup  # Parsing HTML\n",
    "from tika import parser  # parse PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The code below establishes a secure internet connection throughout the scraping process.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T18:53:33.715931Z",
     "start_time": "2020-02-22T18:53:33.708950Z"
    }
   },
   "outputs": [],
   "source": [
    "urllib3.contrib.pyopenssl.inject_into_urllib3()\n",
    "https = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_REQUIRED', ca_certs=certifi.where(), timeout=15.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gets url of years from 1977 to 1997 (first column of letters on website)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T18:53:36.608795Z",
     "start_time": "2020-02-22T18:53:36.605803Z"
    }
   },
   "outputs": [],
   "source": [
    "url_years = [f'https://www.berkshirehathaway.com/letters/{year}.html' for year in range(1977, 1998)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Saves letters into a list.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T18:53:39.517872Z",
     "start_time": "2020-02-22T18:53:38.732846Z"
    }
   },
   "outputs": [],
   "source": [
    "annual_letters = []\n",
    "for i in range(len(url_years)):\n",
    "    annual_letters.append(https.request('GET', url_years[i]).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Writes letters to file.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T18:53:40.227742Z",
     "start_time": "2020-02-22T18:53:40.207753Z"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = '../raw_letters'\n",
    "count = 0\n",
    "for year in range(1977, 1998):\n",
    "    with open(base_dir + '/' + f'{year}_letter.txt', \"wb\") as f:\n",
    "        f.write(annual_letters[count])\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The letters from 1998 to 2019 are in PDF format. The following code attempts to scrape text from PDF files.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.) Create directory for each year of files. Save split PDF files in each directory.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T18:53:43.410404Z",
     "start_time": "2020-02-22T18:53:43.399397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a list of years where each year is a year with an annual report:\n",
    "years = [str(year) for year in range(1998, 2020)]\n",
    "\n",
    "# Note: Change output directory to something else on laptop!\n",
    "output_dir = '../pdf_files'\n",
    "for year in years:\n",
    "    year_directory = os.path.join(output_dir, year)\n",
    "    os.makedirs(year_directory, mode=0o777, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.) Get PDF file from internet.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T18:53:45.181523Z",
     "start_time": "2020-02-22T18:53:45.174541Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_pdf(url, filename):\n",
    "    urllib3.contrib.pyopenssl.inject_into_urllib3()\n",
    "    c = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n",
    "    \n",
    "    with c.request('GET', url, preload_content=False) as resp, open(filename, 'wb') as out_file:\n",
    "        shutil.copyfileobj(resp, out_file)\n",
    "\n",
    "    resp.release_conn()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.) Split PDF file. Save file in year directory.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T18:53:46.865172Z",
     "start_time": "2020-02-22T18:53:46.855199Z"
    }
   },
   "outputs": [],
   "source": [
    "def pdf_splitter(pdfFile):\n",
    "    # creating a pdf file object\n",
    "    pdfFileObj = open(pdfFile, 'rb')\n",
    "\n",
    "    # creating a pdf reader object\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "    for pg in range(pdfReader.numPages):\n",
    "        if len(str(pg)) == 1:\n",
    "            filename = pdfFile.split('.')[0] + '_0' + str(pg) + '.pdf'\n",
    "        else:\n",
    "            filename = pdfFile.split('.')[0] + '_' + str(pg) + '.pdf'\n",
    "        pageObj = pdfReader.getPage(pg)\n",
    "        pdfWriter = PyPDF2.PdfFileWriter()\n",
    "        pdfWriter.addPage(pageObj)\n",
    "        # new pdf file object\n",
    "        newFile = open(filename, 'wb')\n",
    "        pdfWriter.write(newFile)\n",
    "\n",
    "    pdfFileObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4.) Loop through every page to get text. Write text file to new file.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T18:53:49.476026Z",
     "start_time": "2020-02-22T18:53:49.469044Z"
    }
   },
   "outputs": [],
   "source": [
    "def pdf_reader(pdffile):\n",
    "    raw = parser.from_file(pdffile, xmlContent=True)['content']\n",
    "    data = BeautifulSoup(raw, features='html')\n",
    "    message = data.find(class_='page').encode('utf-8')  # for first page\n",
    "    return(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5.) Put it all together.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T19:00:00.379935Z",
     "start_time": "2020-02-22T19:00:00.347021Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    base_dir = '../Milestone_1/'\n",
    "    all_pdf_dict = dict()\n",
    "    urls = ['https://www.berkshirehathaway.com/letters/1998pdf.pdf',\n",
    "            'https://www.berkshirehathaway.com/letters/final1999pdf.pdf',\n",
    "            'https://www.berkshirehathaway.com/letters/2000pdf.pdf',\n",
    "            'https://www.berkshirehathaway.com/letters/2001pdf.pdf',\n",
    "            'https://www.berkshirehathaway.com/letters/2002pdf.pdf']\n",
    "    urls2 = [\n",
    "        f'https://www.berkshirehathaway.com/letters/{year}ltr.pdf' for year in range(2003, 2020)]\n",
    "    urls_combined = urls + urls2\n",
    "\n",
    "    for year in range(1998, 2020):\n",
    "        count = 0\n",
    "        one_pdf_dict = dict()\n",
    "        year_pdf = f'{year}.pdf'\n",
    "        year_dir = base_dir + '/' + str(year)\n",
    "        complete_letter = base_dir + '/' + str(year) + '/' + year_pdf\n",
    "    \n",
    "        for root, dirs, files in os.walk(year_dir):\n",
    "            for name in files:\n",
    "                filename = root + '/' + name\n",
    "                if filename != complete_letter:\n",
    "                    count = count + 1\n",
    "                    one_pdf_dict[str(count)] = pdf_reader(filename)\n",
    "\n",
    "        all_pdf_dict[str(year)] = one_pdf_dict\n",
    "\n",
    "        with open(base_dir + '/raw_letters/' + f'{year}_letter.txt', \"w\") as f:\n",
    "            f.write(str(one_pdf_dict))\n",
    "\n",
    "    return(all_pdf_dict)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
